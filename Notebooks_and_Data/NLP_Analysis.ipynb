{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from datetime import datetime\n",
    "import operator\n",
    "import string\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "path = os.getcwd()\n",
    "path = path + '\\\\NLP_Data\\\\'\n",
    "\n",
    "df = pd.read_csv(path + 'accident_and_injury.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty data cells with Nan\n",
    "df = df.replace('', np.nan)\n",
    "df = df.replace(' ', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change backslashe to whitespace because sentences will be split into words based on whitespaces\n",
    "df['Abstract Text'] = df['Abstract Text'].map(lambda x: x.replace('\\\\', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the time of each event based by locating it in the text, this is done by using regex\n",
    "pattern = '([0-9]{1,2}:[0-9]{2,2}\\s(?:a\\.m\\.|p\\.m\\.))'\n",
    "df['event_time'] = df['Abstract Text'].map(lambda x: re.findall(pattern, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    # If multiple times are found, the first one is taken to be the event time\n",
    "    if (len(df['event_time'].iloc[i]) > 0) and (type(df['event_time'].iloc[i]) == list):\n",
    "        df['event_time'].iloc[i] = str(df['event_time'].iloc[i][0])\n",
    "    # If no times are found, then fill the cell with NaN\n",
    "    elif len(df['event_time'].iloc[i]) == 0:\n",
    "        df['event_time'].iloc[i] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the string format to convert it over to datetime\n",
    "df['event_time'] = df['event_time'].map(lambda x: x.replace('.', '').replace(' ', '') if type(x) != float else x)\n",
    "df['event_time'] = df['event_time'].map(lambda x: datetime.strptime(x, '%I:%M%p').time() if type(x) != float else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_process(text):\n",
    "    # split into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Convert all words to lower case\n",
    "    words = [w.lower() for w in words]\n",
    "\n",
    "    # Remove punctuation from each word\n",
    "    temp = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(temp) for w in words]\n",
    "\n",
    "    # Remove non-alphabetic words\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    # Filter out so called stop words with a built in function\n",
    "    # Example words: the, is, at, which and on\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    # Stemming all words\n",
    "    # Stemming is the process of reducing words to their word stem, base or root form\n",
    "    # Example: cook, cooking and cooked are all stemmed to the word cook\n",
    "    porter = PorterStemmer()\n",
    "    stemmed_words = [porter.stem(word) for word in words]\n",
    "    \n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seperate function for keywords that takes in a single string and returns stemmed words \n",
    "def keyword_process(text):\n",
    "    # Split the string into words on | as seperator\n",
    "    words = re.split(',| ', text)\n",
    "\n",
    "    # Convert all words to lower case\n",
    "    words = [w.lower() for w in words]\n",
    "\n",
    "    # Remove punctuation from each word\n",
    "    temp = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(temp) for w in words]\n",
    "\n",
    "    # Remove non-alphabetic words\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    # Filter out so called stop words with a built in function\n",
    "    # Example words: the, is, at, which and on\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    # Stemming all words\n",
    "    # Stemming is the process of reducing words to their word stem, base or root form\n",
    "    # Example: cook, cooking and cooked are all stemmed to the word cook\n",
    "    porter = PorterStemmer()\n",
    "    stemmed_keywords = [porter.stem(word) for word in words]\n",
    "    \n",
    "    return stemmed_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that cleans a string of words that provide no information on how an accident or injury occured\n",
    "# This function takes care of words that provide no context at all\n",
    "def string_clean(string):\n",
    "    # Define a hard coded list of noisy words\n",
    "    noisewords = ['January','February','March','April','May','June','July', \n",
    "                  'August', 'September', 'October', 'November', 'December',\n",
    "                  'january','february','march','april','may','june','july', \n",
    "                  'august', 'september', 'october', 'november', 'december', \n",
    "                  'a.m.', 'p.m.']\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Remove all noisewords and recombine into a string\n",
    "    cleaned_words  = [word for word in words if word.lower() not in noisewords]\n",
    "    clean_string = ' '.join(cleaned_words)\n",
    "    \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that cleans a list of words that provide no information on how an accident or injury occured\n",
    "# This function takes care of words that might provide some informatino but not in this case\n",
    "def list_clean(word_list):\n",
    "    # Define a hard coded list of noisy words\n",
    "    noise_words = ['employe', 'work', 'approxim', 'cowork', 'right', 'left', 'worker', 'kill', 'sustain', 'hospit', 'oper', \n",
    "                   'suffer', 'die', 'injuri', 'one', 'two', 'three', 'finger', 'head', 'feet', 'amput', 'hand', 'use', \n",
    "                   'back', 'remov', 'cut', 'caus', 'foot']\n",
    "    \n",
    "    # Remove all noisewords from the list\n",
    "    clean_list = [x for x in word_list if x not in noise_words]\n",
    "    \n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(dataframe, clean, event_desc_weight):\n",
    "    ratio_list = list()\n",
    "    overall_list = list()\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        if clean == True:\n",
    "            # Clean the abstract text of words that provide no context\n",
    "            abstract_text = string_clean(dataframe['Abstract Text'].iloc[i])\n",
    "        else:\n",
    "            abstract_text = dataframe['Abstract Text'].iloc[i]\n",
    "\n",
    "        # Get the stemmed words from abstract text, event description and the event keywords\n",
    "        stemmed_text = abstract_process(abstract_text)\n",
    "        stemmed_desc = abstract_process(dataframe['Event Description'].iloc[i])\n",
    "        stemmed_keywords = keyword_process(dataframe['Event Keywords'].iloc[i])\n",
    "\n",
    "        # Get the total number of keywords in the text\n",
    "        number_of_keywords = len(set(stemmed_keywords))\n",
    "\n",
    "        if clean == True:\n",
    "            # Clean the stemmed lists of words extracted from the abstract text and event description\n",
    "            # These words provide no context in this specific scenario\n",
    "            stemmed_text = list_clean(stemmed_text)\n",
    "            stemmed_desc = list_clean(stemmed_desc)\n",
    "\n",
    "        # Get an overall list of words that are common in all texts or lists\n",
    "        overall_list = overall_list + stemmed_text + stemmed_desc\n",
    "\n",
    "        # If any keywords were logged\n",
    "        if number_of_keywords > 0:\n",
    "\n",
    "            # Use built in NLP function to count the frequency of stemmed words in abstract text and the event description\n",
    "            freq_text = nltk.FreqDist(stemmed_text)\n",
    "            freq_desc = nltk.FreqDist(stemmed_desc)\n",
    "\n",
    "            # Increase the weight on event description words because these are often more precise when it comes to the cause\n",
    "            for key in stemmed_desc:    \n",
    "                freq_desc[key] *=  event_desc_weight\n",
    "\n",
    "            # Combine the frequency distributions\n",
    "            freq_combined = { k: freq_text.get(k, 0) + freq_desc.get(k, 0) for k in set(freq_text) | set(freq_desc) }\n",
    "\n",
    "            # Get a list of words with the highest frequency\n",
    "            # The number of words is limited based on the number of keywords, this is done for accuracy comparison\n",
    "            highest_combined = dict(sorted(freq_combined.items(), \n",
    "                                           key = operator.itemgetter(1), reverse = True)[:int(number_of_keywords)])\n",
    "\n",
    "            # Find how many words in the highest frequency table is in the keyword list\n",
    "            intersec = set(highest_combined).intersection(stemmed_keywords)\n",
    "\n",
    "            # Calculate the ratio to use for accuracy predictions\n",
    "            ratio = len(intersec) / number_of_keywords\n",
    "\n",
    "            ratio_list.append(ratio)\n",
    "            \n",
    "    return (np.mean(ratio_list), overall_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy based on original method is  0.2639325579958668 %\n",
      "Accuracy with noise words removed and increased weight on description  0.3659481481277102 %\n"
     ]
    }
   ],
   "source": [
    "org_ratio, org_overall_list = get_ratio(df, False, 1)\n",
    "print('Accuracy based on original method is  {} %'.format(org_ratio)) \n",
    "\n",
    "clean_ratio, clean_overall_list = get_ratio(df, True, 2)\n",
    "print('Accuracy with noise words removed and increased weight on description  {} %'.format(clean_ratio)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last part was used to create the application mockup for health and safety overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fall': 2200,\n",
       " 'fell': 1853,\n",
       " 'struck': 1811,\n",
       " 'truck': 1619,\n",
       " 'fractur': 1273,\n",
       " 'machin': 1107,\n",
       " 'ladder': 1040,\n",
       " 'crush': 952,\n",
       " 'roof': 909,\n",
       " 'caught': 809}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_freq = nltk.FreqDist(clean_overall_list)\n",
    "dict(sorted(main_freq.items(), key=operator.itemgetter(1), reverse=True)[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
